import requests
import re
import urllib.parse as urlparse
from bs4 import BeautifulSoup

class Scanner:

    def __init__(self, url, ignore_links):
        self.session = requests.Session() # maintain a session
        self.target_url = url # base url. this should be the base url if crawling
        self.target_links = []
        self.links_to_ignore = ignore_links

    def request(self, url):
        try:
            get_response = self.session.get(url) # use the same session
            return get_response

        except:  # url doesn't exist
            return 0


    # recursively discover all links related to the website
    def crawl(self, url=None):

        if url == None: # first time calling crawl. Subsequent calls will be recursive
            url = self.target_url

        response = self.request(url)
        if response != 0:  # url is valid

            # extract the links. A list is returned
            href_links = re.findall('(?:href=")(.*?)"', response.text)

            for link in href_links:

                link = urlparse.urljoin(url, link)  # join the relative url (eg. /faq) with the base url

                if "#" in link:  # urls with # in them are going to point to the same page. so get only one of it
                    link = link.split("#")[0]

                # filter out all urls not related to the base url and remove repeated links
                # also ignore all links in the links_to_ignore list
                if self.target_url in link and link not in self.target_links and link not in self.links_to_ignore:

                    self.target_links.append(link)
                    print(link)
                    self.crawl(link)  # recursively call crawl to discover all links on the whole website


    # extract all forms from a given url
    def extract_forms(self, url):
        response = self.request(url)
        forms_list = ""

        if response != 0: # valid url

            # parse the html response using BeautifulSoup.
            # then extract all form tags in the html and store in a list
            parsed_html = BeautifulSoup(response.text, features="html.parser")
            forms_list = parsed_html.findAll("form")

        return forms_list

    # submits a form with a given value to a url
    def submit_form(self, form, value, url):

        action = form.get("action")  # get the action attribute in each extracted form
        post_url = urlparse.urljoin(url, action)
        method = form.get("method")

        # input is a tag within the form tag. It is not an attribute. therefore need to use "findAll" and not "get"
        input_list = form.findAll("input")

        post_data = {}

        for input in input_list:

            input_name = input.get("name")
            input_type = input.get("type")
            input_value = input.get("value")  # data to be submitted

            if input_type == "text":
                input_value = value  # set the data to be submitted

            post_data[input_name] = input_value # this is a dictionary

        if method == "post":
            result = self.session.post(post_url, data=post_data)  # send the data to the website
            return result

        else: # method is get
            result = self.session.get(post_url, params=post_data)
            return result


    def test_xss_in_form(self, form, url):

        xss_test_script = "<sCriPt>alert('test')</sCriPt>"
        response = self.submit_form(form, xss_test_script, url)
        return xss_test_script in response.text # true or false


    def test_xss_in_link(self, url):

        xss_test_script = "<sCriPt>alert('test')</sCriPt>"
        url = url.replace("=", "=" + xss_test_script)
        response = self.session.get(url)
        return xss_test_script in response.text # true or false


    # iterate over all links in target_links, scanning each one for XSS
    def run (self):

        for link in self.target_links:
            forms = vuln_scanner.extract_forms(link) # this is a list of forms in the link

            for form in forms:

                print("[+] Testing form in " + link)
                is_vulnerable_to_xss = self.test_xss_in_form(form, link)
                if is_vulnerable_to_xss:
                    print("\n\n[***] XSS discovered in " + link + " in the following form: \n")
                    print(str(form) + "\n\n")


            if "=" in link: # this is a GET request. it sends data
                print("[+] Testing " + link)
                is_vulnerable_to_xss = self.test_xss_in_link(link)
                if is_vulnerable_to_xss:
                    print("\n\n[***] XSS discovered in " + link)


target_url = "http://10.0.2.23/dvwa/"
links_to_ignore = ["http://10.0.2.23/dvwa/logout.php"] # don't send requests to this otherwise session will log out

# "username", "password" and "Login" is the name of the fields in the login form
# "Login" is the button to submit the form
website_data = {"username": "admin", "password": "password", "Login": "submit"}

vuln_scanner = Scanner(target_url, links_to_ignore)

# login with the given username and password, and also maintain the session before crawling
vuln_scanner.session.post("http://10.0.2.23/dvwa/login.php", data=website_data)

vuln_scanner.crawl()
vuln_scanner.run()